<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <title>随笔</title>
    <link rel="stylesheet" href="../../../../css/style.css">
    <link rel="stylesheet" href="../../../../css/gitment.css">
    <link rel="stylesheet" type="text/css" href="//at.alicdn.com/t/font_859455_eaq7v6w8ktj.css">
</head>
<body>
<header class="header">
    <div class="header-inner">
        <div class="header-title">

        </div>
        <nav class="header-nav">
            
            <a href="../../../../index.html" class="header-nav-link">
                首页
            </a>
            

            
            <a href="../../../../archives" class="header-nav-link">
                归档
            </a>
            

            
            <a href="../../../../tags" class="header-nav-link">
                标签
            </a>
            

            
        </nav>
    </div>
</header>
<header class="mobile-header">
    <div class="mobile-nav">
        <div class="mobile-nav-icon">
            <span></span>
            <span></span>
            <span></span>
        </div>
        <div class="mobile-nav-title">
            <a href="../../../../index.html" class="mobile-nav-title-link">Jun's Blog</a>
        </div>

    </div>
    <nav class="mobile-menu">
        <ul class="mobile-menu-list">
            <li class="mobile-menu-item">
                <i class="iconfont icon-home"></i>
                <a href="../../../../index.html" class="mobile-nav-link">首页</a>
            </li>
            <li class="mobile-menu-item">
                <i class="iconfont icon-archive"></i>
                <a href="../../../../archives" class="mobile-nav-link">归档</a>
            </li>
            <li class="mobile-menu-item">
                <i class="iconfont icon-tag"></i>
                <a href="../../../../tags" class="mobile-nav-link">标签</a>
            </li>
            <li class="mobile-menu-item">
                <i class="iconfont icon-about"></i>
                <a href="/about/" class="mobile-nav-link">关于</a>
            </li>
        </ul>
    </nav>
</header>
<div class="main">
    <div class="content-inner">
        <div class="posts">
    <article class="post-whole">
        <div class="post-title">
            <h2 class="title">使用Scrapy爬取马蜂窝游记</h2>
            <div class="post-meta">
                <span class="post-time">2019-03-17</span>
                
                <span class="post-category">
                    
                    <a class="category" href="../../../../categories/爬虫项目/">爬虫项目</a>
                    
                </span>
                
                <span class="post-visit"> 阅读次数：<span id="busuanzi_value_page_pv"></span></span>
            </div>
        </div>
        <div class="post-toc" id="post-toc">
    <strong class="post-toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#开始项目"><span class="toc-text">开始项目</span></a></li></ol>
    <div class="back-to-top" id="back-to-top">
        <a href="javascript:void(0);">回到顶部</a>
    </div>
</div>
        <div class="post-content">
            <p><strong>简介：抓取每一个月的每一个地点的每一个用户的游记的简要信息，这里只简单的定义了要爬取的字段（用户名，标题，简述，点赞数</strong>）</p>
<p>项目GitHub地址：<a href="https://github.com/luckyWu/Spider-project.git" target="_blank" rel="noopener">https://github.com/luckyWu/Spider-project.git</a></p>
<hr>
<p><strong>先浏览下网页</strong></p>
<p><strong>1.打开马蜂窝网站选择目的地页面（如下图）</strong></p>
<p>时间一共有12个月，每一月旅游地点又分成多页显示</p>
<a id="more"></a>
<img src="/2019/03/17/使用Scrapy爬取马蜂窝数据/20190304154107.png">)<br><br>—–<br><br><strong>2.通过观察每一页的地名都是通过AJAX来加载的</strong><br><br><img src="/2019/03/17/使用Scrapy爬取马蜂窝数据/20190304171645.png">
<hr>
<p><strong>3.任选一个目的地点入,往下面滑动将看到用户的旅游日记</strong></p>
<img src="/2019/03/17/使用Scrapy爬取马蜂窝数据/20190304154835.png">
<p><strong>上图中的每一个游记的简介便是要抓取的信息</strong></p>
<hr>
<h3 id="开始项目"><a href="#开始项目" class="headerlink" title="开始项目"></a>开始项目</h3><p><strong>1.安装scrpay </strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ubuntu 安装</span><br><span class="line">sudo apt-get install build-essential python3-dev libssl-dev libffi-dev libxml2</span><br><span class="line">libxml2-dev libxslt1-dev zlib1g-dev</span><br><span class="line"></span><br><span class="line">pip3 install Scrapy</span><br><span class="line">-----------------------------------------------------------------------------------------------</span><br><span class="line">windows 安装</span><br><span class="line">安装包下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</span><br><span class="line">1.首先下载whl包</span><br><span class="line">下载Scrapy-1.5.1-py2.py3-none-any.whl</span><br><span class="line">2.安装wheel库的请先安装pip install wheel</span><br><span class="line">3.scrapy依赖twiste</span><br><span class="line">下载Twisted-18.9.0-cp37-cp37m-win_amd64.whl</span><br><span class="line">4.scrapy依赖lxml包</span><br><span class="line">pip install lxml</span><br><span class="line">如遇到需要下载 pywin32，请下载安装</span><br><span class="line">pip install pypiwin32</span><br><span class="line">6.安装pip install scrapy</span><br></pre></td></tr></table></figure>
<p><strong>2.创建项目</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#在命令提示符下输入</span><br><span class="line">scrapy startproject myscrapy  #myscrapy为项目名称</span><br></pre></td></tr></table></figure>
<p><strong>3.进入项目创建spider</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider football www.mafengwo.cn</span><br></pre></td></tr></table></figure>
<p><strong>4.用pycharm打开</strong></p>
<img src="/2019/03/17/使用Scrapy爬取马蜂窝数据/20190304162004.png">
<p><strong>5.编写item</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class MyscrapyItem(scrapy.Item):</span><br><span class="line"></span><br><span class="line">    title = scrapy.Field() # 标题</span><br><span class="line">    content = scrapy.Field() # 简介</span><br><span class="line">    zan = scrapy.Field() #获赞</span><br><span class="line">    user_name = scrapy.Field() #用户名</span><br></pre></td></tr></table></figure>
<p><strong>6.配置MySQL，使用pymysql连接</strong></p>
<ul>
<li>创建数据库时要设置utf8bm4格式， 因为抓取的数据中有图形符号字符</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line">def get_con():</span><br><span class="line">    &quot;&quot;&quot;连接数据库&quot;&quot;&quot;</span><br><span class="line">    host = &apos;127.0.0.1&apos;</span><br><span class="line">    port = 3306</span><br><span class="line">    user = &apos;root&apos;</span><br><span class="line">    password = &apos;123456&apos;</span><br><span class="line">    database = &apos;mfw_trip1&apos;</span><br><span class="line">    db = pymysql.connect(host=host, user=user, password=password, database=database, charset=&apos;utf8mb4&apos;, port=port)</span><br><span class="line">    return db</span><br><span class="line"></span><br><span class="line">def get_cursor(db):</span><br><span class="line">    &quot;&quot;&quot;获取游标对象&quot;&quot;&quot;</span><br><span class="line">    cursor = db.cursor()</span><br><span class="line">    return cursor</span><br><span class="line"></span><br><span class="line">def insert(db, cursor, item):</span><br><span class="line">    &quot;&quot;&quot;插入数据&quot;&quot;&quot;</span><br><span class="line">    cursor.execute(</span><br><span class="line">        query=&apos;insert into commits(title, content, zan, user_name) values (%s,%s,%s,%s)&apos;,</span><br><span class="line">        args=( item[&apos;title&apos;], item[&apos;content&apos;], item[&apos;zan&apos;], item[&apos;user_name&apos;]))</span><br><span class="line">    db.commit()</span><br></pre></td></tr></table></figure>
<p><strong>7.setting配置</strong></p>
<ul>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">robots要设置为False, 重试次数使用的话通过scrapy.contrib.downloadermiddleware.retry.RetryMiddleware配置</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置logging记录错误</p>
<p>错误级别有5个</p>
<p>CRITICAL - 严重错误 </p>
<p>ERROR - 一般错误 </p>
<p>WARNING - 警告信息 </p>
<p>INFO - 一般信息 </p>
<p>DEBUG - 调试信息</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import datetime</span><br><span class="line">today = datetime.datetime.now()</span><br><span class="line">log_file_path = &quot;log/&#123;&#125;-&#123;&#125;-&#123;&#125;.log&quot;.format(today.year, today.month, today.day)</span><br><span class="line">LOG_LEVEL= &apos;WARNING&apos;</span><br><span class="line">LOG_FILE =log_file_path</span><br></pre></td></tr></table></figure>
<p><strong>8.编写spider</strong></p>
<ul>
<li><p>通过观察地点列表和游记列表都是发ajax的POST请求加载的，都有请求参数，通过观察地点列表的ajax请求的参数和月份和有关其中 ‘tag[]’ = 113 + month（初始为0） * 3, page（初始为1）的值则可以通过下一页的链接取到，</p>
</li>
<li><p>游记列表的关键请求参数有’mddid’,  ‘page’,  ‘_ts’,  ‘_sn’,其中mddid代表一个城市，ts看起来像时间戳的格式，笔者用随机数伪造，sn用随机字符伪造</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line">import json </span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line">from lxml import etree</span><br><span class="line">import re</span><br><span class="line">from scrapy import Spider, Request</span><br><span class="line">from scrapy import FormRequest</span><br><span class="line">from myscrapy.items import MyscrapyItem</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">ALL_CHARS = &apos;0123456789abcdefghijklmnopqrstuvwxyz&apos;</span><br><span class="line"></span><br><span class="line">def random_sn():</span><br><span class="line">    &quot;&quot;&quot;生成随机字符串&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    s = &apos;&apos;</span><br><span class="line">    chars_len = len(ALL_CHARS)</span><br><span class="line">    for _ in range(10):</span><br><span class="line">        index = random.randrange(chars_len)</span><br><span class="line">        s += (ALL_CHARS[index])</span><br><span class="line">    return s</span><br><span class="line"></span><br><span class="line">def random_ts():</span><br><span class="line">    &quot;&quot;&quot;伪造随机时间戳&quot;&quot;&quot;</span><br><span class="line">    str = &apos;0123456789&apos;</span><br><span class="line">    s = [&apos;155&apos;, &apos;153&apos;]</span><br><span class="line">    k = random.choice(s) + &apos;&apos;.join(random.choice(str) for i in range(10))</span><br><span class="line">    return k</span><br><span class="line"></span><br><span class="line">class FootballSpider(Spider):</span><br><span class="line">    &quot;&quot;&quot;解析网页&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    name = &apos;football&apos;</span><br><span class="line">    allowed_domains = [&apos;mafengwo.cn&apos;]</span><br><span class="line">    start_urls = [&apos;http://mafengwo.cn/&apos;]</span><br><span class="line">    every_month_url = &apos;http://www.mafengwo.cn/mdd/base/filter/getlist&apos;</span><br><span class="line">    travel_url = &apos;http://www.mafengwo.cn/gonglve/ajax.php?act=get_travellist&apos;</span><br><span class="line">    base_month = 113</span><br><span class="line">    params = &#123;</span><br><span class="line">        &apos;mddid&apos;: &apos;10189&apos;,</span><br><span class="line">        &apos;pageid&apos;: &apos;mdd_index&apos;,</span><br><span class="line">        &apos;sort&apos;: &apos;1&apos;,</span><br><span class="line">        &apos;cost&apos;: &apos;0&apos;,</span><br><span class="line">        &apos;days&apos;: &apos;0&apos;,</span><br><span class="line">        &apos;month&apos;: &apos;0&apos;,</span><br><span class="line">        &apos;tagid&apos;: &apos;0&apos;,</span><br><span class="line">        &apos;page&apos;:&apos;1&apos;,</span><br><span class="line">        &apos;_ts&apos;: random_ts(),</span><br><span class="line">        &apos;_sn&apos;: random_sn(),</span><br><span class="line">    &#125;</span><br><span class="line">    month_params = &#123;</span><br><span class="line">        &apos;tag[]&apos;: &apos;113&apos;,</span><br><span class="line">        &apos;page&apos;: &apos;1&apos;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">    	&quot;&quot;&quot;重写starts_requests()函数生成请求&quot;&quot;&quot;</span><br><span class="line">        for month in range(0, 13):</span><br><span class="line">            # 构建请求参数</span><br><span class="line">            tag = 113 + month * 3</span><br><span class="line">            month_num_params = self.month_params</span><br><span class="line">            month_num_params[&apos;tag[]&apos;] = str(tag)</span><br><span class="line">            yield FormRequest(self.every_month_url, callback=self.parse, 			      formdata=month_num_params)</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        &quot;&quot;&quot;解析每一页的每一个地点&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        if response.status == 200 :</span><br><span class="line">            res = json.loads(response.text)</span><br><span class="line">            s = res.get(&apos;list&apos;, 0)</span><br><span class="line"></span><br><span class="line">            if s:</span><br><span class="line">                page_month = res.get(&apos;page&apos;, 0)</span><br><span class="line">                # 提取下一页的请求参数</span><br><span class="line">                obj = re.compile(r&apos;&lt;a rel=&quot;nofollow&quot; data-page=&quot;(\d+)&quot; href&apos;) </span><br><span class="line">                obj1 = re.search(obj, page_month)</span><br><span class="line">                html = etree.HTML(s)</span><br><span class="line">                lis = html.xpath(&apos;//li[@class=&quot;item&quot;]&apos;)</span><br><span class="line">                for li in lis[:]:</span><br><span class="line">                    href = li.xpath(&apos;./div[@class=&quot;img&quot;]/a/@href&apos;)[0]</span><br><span class="line">                    print(href,&apos;show href--------&apos;)</span><br><span class="line">                    mdds = href.split(&apos;/&apos;)[-1]</span><br><span class="line">                    mdd = mdds.split(&apos;.&apos;)[0]</span><br><span class="line">                    month_page_params = self.params</span><br><span class="line">                    month_page_params[&apos;mddid&apos;] = mdd</span><br><span class="line">                    yield FormRequest(self.travel_url, callback=self.parse_travellist, formdata=month_page_params)</span><br><span class="line">                if obj1:</span><br><span class="line">                    month_page_next_params = self.params</span><br><span class="line">                    month_page_next_params[&apos;page&apos;] = obj1.group(1)</span><br><span class="line">                    yield FormRequest(self.every_month_url, callback=self.parse,</span><br><span class="line">                                      formdata=month_page_next_params)</span><br><span class="line">        logging.warning(&quot;parse_list失败&quot;)</span><br><span class="line"></span><br><span class="line">    def parse_travellist(self, response):</span><br><span class="line">        &quot;&quot;&quot;得到每一个地点的游记列表&quot;&quot;&quot;</span><br><span class="line">        # print(&quot;into parse_travellist************************************&quot;)</span><br><span class="line">        if response.status == 200 :</span><br><span class="line">            print(&quot;enter&quot;)</span><br><span class="line">            res = response.text</span><br><span class="line">            content = json.loads(res)</span><br><span class="line">            s = content.get(&apos;list&apos;, 0)</span><br><span class="line">            page_info = content.get(&apos;page&apos;,0)</span><br><span class="line">            #提取下一页的请求参数</span><br><span class="line">            next = re.compile(r&apos;&lt;a class=&quot;pi pg-next&quot; href=&quot;/yj/(\d+)/1-0-(\d+).html&quot; title&apos;)</span><br><span class="line">            next = re.search(next,page_info)</span><br><span class="line"></span><br><span class="line">            if s:</span><br><span class="line">                s = &quot;&lt;html&gt;&quot; + s + &quot;&lt;/html&gt;&quot;</span><br><span class="line">                html = etree.HTML(s)</span><br><span class="line">                lis = html.xpath(&apos;//div[@class=&quot;tn-item clearfix&quot;]&apos;)</span><br><span class="line">                for li in lis[:]:</span><br><span class="line">                    href1 = li.xpath(&apos;.//a[@class=&quot;title-link&quot;]//@href&apos;)[0]</span><br><span class="line">                    title = li.xpath(&apos;.//a[@class=&quot;title-link&quot;]/text()&apos;)#(&apos;./div[@class=&quot;tn-wrapper&quot;]/dl/dt/a/text()&apos;)</span><br><span class="line">                    content = li.xpath(&apos;./div[@class=&quot;tn-wrapper&quot;]/dl/dd/a/text()&apos;)</span><br><span class="line">                    zan = li.xpath(&apos;./div[@class=&quot;tn-wrapper&quot;]/div/span[@class=&quot;tn-ding&quot;]/em/text()&apos;)</span><br><span class="line">                    user_name = li.xpath(&apos;./div[@class=&quot;tn-wrapper&quot;]/div/span[@class=&quot;tn-user&quot;]/a/text()&apos;)</span><br><span class="line">                    item = MyscrapyItem()</span><br><span class="line">                    item[&apos;title&apos;] = title[0] if title else &apos;&apos;</span><br><span class="line">                    item[&apos;content&apos;] = content[0] if content else &apos;&apos;</span><br><span class="line">                    item[&apos;zan&apos;] = zan[0] if zan else &apos;&apos;</span><br><span class="line">                    item[&apos;user_name&apos;] = user_name[0] if user_name else &apos;&apos;</span><br><span class="line">                    yield item</span><br><span class="line">                    # url = &apos;http://www.mafengwo.cn&apos; + href1</span><br><span class="line">                    # yield Request(url, callback=self.parse_detail, dont_filter=False)</span><br><span class="line">                if next:</span><br><span class="line">                    next_page = next.group(1) #获取参数midde的值</span><br><span class="line">                    next_num = next.group(2) #获取参数page的值</span><br><span class="line">                    every_page_params = self.params</span><br><span class="line">                    every_page_params[&apos;mddid&apos;] = next_page</span><br><span class="line">                    every_page_params[&apos;page&apos;] = next_num</span><br><span class="line">                    yield FormRequest(self.travel_url, callback=self.parse_travellist,dont_filter=False, formdata=every_page_params)</span><br><span class="line">            else:</span><br><span class="line">                logging.warning(f&quot;parse_travellist失败！：&#123;response.status&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    # def parse_detail(self, response):</span><br><span class="line">    #     &quot;&quot;&quot;每一个游记的详细内容&quot;&quot;&quot;</span><br><span class="line">    #     print(response.text)</span><br></pre></td></tr></table></figure>
<p><strong>9.middlewares</strong></p>
<p>笔者爬取前就买了蘑菇代理准备对付封IP, 后来没用代理IP效果感觉也挺好就没用了</p>
<p>笔者买的蘑菇代理每次有提取频率限制，提取后还要等一会才能重新提取，为了充分利用每一个代理IP，笔者用队列来保存代理IP，每次从队列取出一个直到该ip失效再重取，下面是蘑菇代理的调用函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">#from fake_useragent import UserAgent</span><br><span class="line"></span><br><span class="line">q = deque(maxlen=50)</span><br><span class="line"></span><br><span class="line">def getip():</span><br><span class="line">    time.sleep(10)</span><br><span class="line">    url=&apos;your key&apos;</span><br><span class="line">    # ip数小于3添加ip</span><br><span class="line">    if len(q) &lt; 3:</span><br><span class="line">        res = requests.get(url)</span><br><span class="line">        ips = (res.content.decode(&apos;utf-8&apos;))</span><br><span class="line">        if re.match(r&apos;\d+\.&apos;, ips):</span><br><span class="line">            ip = ips.split(&apos;\r\n&apos;)</span><br><span class="line">            for i in ip[:5]:</span><br><span class="line">                proxy = &apos;http://&apos;+i</span><br><span class="line">                q.append(proxy)</span><br><span class="line"> curent_ip = pop()</span><br></pre></td></tr></table></figure>
<p><strong>用Navicat连上数据库看一看效果</strong></p>
<img src="/2019/03/17/使用Scrapy爬取马蜂窝数据/mfw_mysql_data.PNG">

        </div>
        
        <div class="post-tag">
            
            <a class="tag" href="../../../../tags/Scrapy/" title="Scrapy">Scrapy</a>
            
        </div>
        
    </article>
</div>
<div class="paginator">
    
        
            <a class="prev" href="../img/">
                <i class="iconfont icon-prev"></i>
                <span class="nav-default">img</span>
                <span class="nav-mobile">上一篇</span>
            </a>
        
        
            <a class="next" href="../../../../2018/10/29/js/">
                <span class="nav-default">hello,</span>
                <span class="nav-mobile">下一篇</span>
                <i class="iconfont icon-next"></i>
            </a>
        
    
</div>
<div id="comment-container"></div>
    </div>
</div>
<footer class="footer-social">
    

    

    

    <div class="footer-copyright">
        <p class="time-line">
            &copy;
            
            
            2019
            &nbsp;<i class="iconfont icon-heart"></i>&nbsp;
            <a target="_blank" href="https://github.com/iJinxin">Jun</a>
        </p>
        <p class="theme-info">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Theme -
            <a target="_blank" href="https://github.com/iJinxin/hexo-theme-sky">Sky</a>
        </p>
    </div>
</footer>
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="../../../../js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
    

</script>
</html>
